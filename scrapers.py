# File: scrapers.py

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import re
import time

def setup_driver():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Selenium Driver ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á"""
    service = Service(executable_path='chromedriver.exe')
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--log-level=3') # ‡∏ã‡πà‡∏≠‡∏ô log ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    driver = webdriver.Chrome(service=service, options=options)
    return driver

def scrape_one31_schedule():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å ONE31"""
    URL = "https://www.one31.net/schedule"
    print("üïµÔ∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡πà‡∏≠‡∏á ONE31...")
    driver = setup_driver()
    scraped_programs = []
    
    try:
        driver.get(URL)
        try:
            cookie_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler")))
            cookie_button.click()
            time.sleep(2)
        except TimeoutException:
            pass # ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡∏Å‡πá‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£

        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, 'current-sc-day')))
        html = driver.page_source
        
        soup = BeautifulSoup(html, 'html.parser')
        main_container = soup.find('div', class_='tab-content')
        if main_container:
            program_items = main_container.find_all('p', class_='card-text')
            for item in program_items:
                title_tag = item.find('h2', class_='title')
                time_detail_tag = item.find('div', class_='subtitle-detail-sch')
                if title_tag and time_detail_tag:
                    title = title_tag.text.strip()
                    time_text = time_detail_tag.text.strip()
                    match = re.search(r'‡πÄ‡∏ß‡∏•‡∏≤\s*(\d{2}:\d{2})', time_text)
                    if match:
                        scraped_programs.append({"start_time": match.group(1), "title": title})
            print("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ONE31 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ONE31: {e}")
    finally:
        driver.quit()
    return scraped_programs

# In scrapers.py file

# In scrapers.py file

def scrape_mono29_schedule():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å MONO29 (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô)"""
    URL = "https://mono29.com/schedule"
    print("üïµÔ∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡πà‡∏≠‡∏á MONO29...")
    driver = setup_driver()
    scraped_programs = []

    try:
        driver.get(URL)
        try:
            cookie_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, "pdpa_cookies_btn_accept")))
            print("üëç MONO29: ‡πÄ‡∏à‡∏≠‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏Å‡∏î...")
            cookie_button.click()
            time.sleep(2)
        except TimeoutException:
            pass 

        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, 'container-schedule')))
        html = driver.page_source

        soup = BeautifulSoup(html, 'html.parser')
        
        # --- ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ---
        # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏•‡πà‡∏≠‡∏á‡πÉ‡∏´‡∏ç‡πà 'container-schedule' ‡∏Å‡πà‡∏≠‡∏ô
        main_container = soup.find('div', class_='container-schedule')
        
        # 2. ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡∏Ñ‡πà‡∏≠‡∏¢‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ' ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ ID ‡∏ß‡πà‡∏≤ 'loadContents-1'
        today_schedule_container = main_container.find('div', id='loadContents-2')

        if today_schedule_container:
            program_items = today_schedule_container.find_all('div', class_='programInDisplay')
            for item in program_items:
                try:
                    time_tag = item.find('div', class_='box-time')
                    img_tag = item.find('img')
                    
                    time_str = time_tag.text.strip().replace(' ‡∏ô.', '')
                    title = img_tag['alt'].strip()

                    if time_str and title:
                        scraped_programs.append({"start_time": time_str, "title": title})
                except (AttributeError, KeyError):
                    continue
            print("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MONO29 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ)")
        else:
            print("‚ùå MONO29: ‡πÑ‡∏°‡πà‡∏û‡∏ö container ‡∏Ç‡∏≠‡∏á‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (id='loadContents-1')")

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MONO29: {e}")
    finally:
        driver.quit()
    return scraped_programs


# In scrapers.py file, replace the old thairath function with this one

import requests
from bs4 import BeautifulSoup
import json

def scrape_thairath_schedule():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å Thairath TV (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç KeyError)"""
    URL = "https://www.thairath.co.th/tv/schedule"
    print("üïµÔ∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡πà‡∏≠‡∏á Thairath (‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏´‡∏°‡πà)...")
    scraped_programs = []

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(URL, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        next_data_script = soup.find('script', id='__NEXT_DATA__')
        
        if not next_data_script:
            print("‚ùå Thairath: ‡πÑ‡∏°‡πà‡∏û‡∏ö script tag '__NEXT_DATA__'")
            return []

        data = json.loads(next_data_script.string)
        
        # --- ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
        schedule_parts = data['props']['initialState']['tv']['data']['items']['schedule'][0]

        for part_name, part_data in schedule_parts.items():
            for item in part_data.get('items', []):
                title = item.get('title')
                start_time = item.get('onAirTime')

                if title and start_time:
                    scraped_programs.append({"start_time": start_time, "title": title})
        
        print("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Thairath ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Thairath: {e}")
    
    return scraped_programs

# In scrapers.py file, add this new function at the end

def scrape_ch3_schedule():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å CH3 Plus"""
    URL = "https://ch3plus.com/schedule"
    print("üïµÔ∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡πà‡∏≠‡∏á 3HD...")
    driver = setup_driver()
    scraped_programs = []

    try:
        driver.get(URL)
        try:
            # ‡∏£‡∏≠‡πÅ‡∏•‡∏∞‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ "‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
            cookie_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, "//button[text()='‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î']")))
            print("üëç CH3: ‡πÄ‡∏à‡∏≠‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏Å‡∏î...")
            cookie_button.click()
            time.sleep(2)
        except TimeoutException:
            pass # ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡∏Å‡πá‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£

        # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, 'schedule-detail')))
        html = driver.page_source

        soup = BeautifulSoup(html, 'html.parser')
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
        schedule_table = soup.find('table', class_='schedule-detail')
        if schedule_table:
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô <tr>
            program_items = schedule_table.find('tbody').find_all('tr')
            for item in program_items:
                try:
                    columns = item.find_all('td')
                    if len(columns) == 2:
                        # ‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏£‡∏Å, ‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á
                        time_str = columns[0].text.split('-')[0].strip() # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°
                        title = columns[1].text.strip()
                        
                        # ‡∏ï‡∏±‡∏î element ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏¥‡πâ‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏π‡∏õ‡πÑ‡∏≠‡∏Ñ‡∏≠‡∏ô)
                        if "‡∏î‡∏π‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢" in title:
                            title = title.replace("‡∏î‡∏π‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢", "").strip()
                        if "‡∏î‡∏π‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏ó‡∏µ‡∏ß‡∏µ" in title:
                            title = title.replace("‡∏î‡∏π‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏ó‡∏µ‡∏ß‡∏µ", "").strip()

                        if time_str and title:
                            scraped_programs.append({"start_time": time_str, "title": title})
                except (AttributeError, IndexError):
                    continue
            print("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 3HD ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 3HD: {e}")
    finally:
        driver.quit()
    return scraped_programs

# In scrapers.py file, replace the old amarin function with this one

#def scrape_amarin_schedule():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å Amarin TV (‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)"""
    URL = "https://www.amarintv.com/schedule"
    print("üïµÔ∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡πà‡∏≠‡∏á Amarin TV (Selenium)...")
    driver = setup_driver()
    scraped_programs = []

    try:
        driver.get(URL)
        
        print("...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à...")
        
        # ‡∏£‡∏≠‡∏à‡∏ô‡πÄ‡∏à‡∏≠ "‡∏õ‡∏∏‡πà‡∏°" ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
        time_section_buttons = WebDriverWait(driver, 20).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "button[id^=':r']"))
        )

        print(f"üëç‡πÄ‡∏à‡∏≠‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° {len(time_section_buttons)} ‡∏õ‡∏∏‡πà‡∏° ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
        for button in time_section_buttons:
            try:
                # ‡πÉ‡∏ä‡πâ JavaScript ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                driver.execute_script("arguments[0].click();", button)
                time.sleep(1) # ‡∏£‡∏≠ 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏•‡∏¥‡∏Å
            except Exception:
                continue # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏•‡∏¥‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡πá‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ

        # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏õ‡∏¥‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡∏Ñ‡πà‡∏≠‡∏¢‡∏î‡∏∂‡∏á‡πÇ‡∏Ñ‡πâ‡∏î HTML ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏°‡∏≤‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡πâ‡∏ß)
        program_items = soup.find_all('a', class_='detail_row__jWZ4z')
        
        if program_items:
            for item in program_items:
                try:
                    time_str = item.find_all('div')[0].text.strip()
                    title = item.find_all('div')[2].text.strip()

                    if time_str and title:
                        scraped_programs.append({"start_time": time_str, "title": title})
                except (AttributeError, IndexError):
                    continue
            print("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Amarin TV ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        else:
            print("‚ùå Amarin TV: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏¢‡πà‡∏≠‡∏¢")

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Amarin TV: {e}")
    finally:
        driver.quit()
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö
    scraped_programs.sort(key=lambda x: x['start_time'])
    return scraped_programs

# In scrapers.py file, replace the old ch7 function with this new one

# def scrape_ch7_schedule():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å CH7 (‡πÉ‡∏ä‡πâ Selenium ‡πÅ‡∏•‡∏∞ Class ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ)"""
    URL = "https://www.ch7.com/schedule.html"
    print("üïµÔ∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡πà‡∏≠‡∏á 7HD (Selenium)...")
    driver = setup_driver()
    scraped_programs = []

    try:
        driver.get(URL)
        
        try:
            # --- ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç 1: ‡∏£‡∏≠‡πÅ‡∏•‡∏∞‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡∏ï‡∏≤‡∏° ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
            print("...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏õ‡∏∏‡πà‡∏°‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ...")
            cookie_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.ID, "cookie-notice-accept-btn"))
            )
            print("üëç 7HD: ‡πÄ‡∏à‡∏≠‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏Å‡∏î...")
            # ‡πÉ‡∏ä‡πâ Javascript ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
            driver.execute_script("arguments[0].click();", cookie_button) 
            time.sleep(2) # ‡∏£‡∏≠ 2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ pop-up ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
        except TimeoutException:
            print("ü§î 7HD: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ")

        # --- ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç 2: ‡∏£‡∏≠‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ 'content-left' ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à ---
        print("...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏±‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à...")
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CLASS_NAME, "content-left"))
        )

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ class 'content-left'
        program_items = soup.find_all('div', class_='content-left')
        
        for item in program_items:
            try:
                # --- ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç 3: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Class ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
                time_str = item.find(class_='text-muted').text.strip()
                title = item.find(class_='text-title-schedule-program').text.strip()

                if time_str and title:
                    scraped_programs.append({"start_time": time_str, "title": title})
            except (AttributeError, IndexError):
                continue
        print("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 7HD ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 7HD: {e}")
    finally:
        driver.quit()
    
    return scraped_programs
# ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô scrape_...() ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ